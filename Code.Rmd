---
title: 'A comprehensive statistical and machine learning exercise'
author: "Tina Rezvanian"
date: "June 3, 2019"
output:
  html_document: default
  pdf_document: default
---

<style>
body {
text-align: justify}
</style>


```{r setup, include=FALSE}
options(warn=-1)

library(ggplot2)
library(Hmisc)
library(dplyr)
library(car)
library(MLmetrics)
library(rpart)
library(rpart.plot)
library(randomForest)
library(MASS)
library(AER)
library(pscl)
library(lime)
library(vip)        
library(pdp)       
library(ggplot2)    
library(caret)      
library(ranger)
library(dplyr)  
library(h2o)
#library(iml)
library(xgboost)
library(plyr)
library(mlr)
#library(ltm)
library(tidyr)


options( warn = -1 )

```

## 1. Question 1: 

Can you create the summary statistics and histogram of Days Open on a federal case? The number of days open can be calculated as Days Open = TERMINATION_DATE – FILING_DATE.

First, I start by attaching the required libraries. I read the dataset. The data contains 457669 observations and 25 columns. I check if the data set contains any NA, NaN, or infinite value.  It turns out that the only column with NA variables is the TRANSFER_DATE column, which correspond to cases that are not transfered. I check each variables range and format and then, I divide data columns into factor variables, continuous variables, and date variables in var_factor, var_cont, and var_factor, respectively, based on data type and definitions in the codebook. I create OPEN_DAYS as TERMINATION_DATE – FILING_DATE. OPEN_DAYS is a dependent variable and TERMINATION_DATE  and FILING_DATE are independent variables in this question. The filing date of all cases is known, however, the termination date of pending cases is not known, as they are not terminated yet. In the data, 1900-01-01 is the termination date listed for pending cases. Therefore, in the OPEN_DAYS column, their respective number of open days has a negative value where number of days a case is open cannot be negative. Also, the value of Open Days for pending cases is not meaningful since their termination date cannot be 1900-01-01. We can assume that the OPEN_DAYS is NULL for pending cases. As a result, looking at the statistics of Open Days for all cases would not be useful because OPEN_DAYS values of pending cases is not meaningful and would deviate the results.  

Looking at the summary statistics, the number of open days for closed cases lies between 0 and 11555 days. On average, cases have been open for 248.5 days. 50% of the cases have been open for less than 359.2 days. It also shows that most cases (75% of cases) have been open between 92 and 512 days.
In the plot titled “Frequency Distribution: Open Days for All Cases” the histogram of Open Days clearly separates pending cases and closed cases. The plot “Frequency Distribution: Open Days for Closed Cases” is a better way of looking at the histogram of Open Days. 
Looking at the histogram bins and the number of cases in each bin, most cases has been open less than 5000 days, and very few cases have been open for more than 6000 days. 
I am also plotting the box plot to show where the range of open days are for 75% of cases and also to show the outliers. 


```{r  data_reading, out.width = "50%",  fig.show='hold'}
df <- readRDS('./data_exercise_short.rds')
names(df)
indx <- apply(df, 2, function(x) any(is.nan(x)| is.na(x) | is.infinite(x)))
names(df)[indx]   # "TRANSFER_DATE"

#check missing values
# only TRANSFER_DATE has na' s: the rest are shown with -8 
# no need to impute, I will generate a new variable as to show ehather uis was transfered (binary variable)
var_factor <- c("CIRCUIT","DISTRICT","ORIGIN","JURISDICTION","DIVERSITY_RESIDENCE" ,"JURY_DEMAND",
                "COUNTY_OF_RESIDENCE","PLAINTIFF","DEFENDANT","PROCEDURAL_PROGRESS","DISPOSITION","NATURE_OF_JUDGMENT" ,     
                "JUDGEMENT","PRO_SE","FEE_STATUS","STATUS_CODE","YEAR_OF_TAPE","id")

for (i in var_factor){
  df[[i]] <- as.factor(df[[i]])
}

var_date <- c("FILING_DATE", "TRANSFER_DATE" , "TERMINATION_DATE")

for (i in var_date){
  df[[i]] <- as.Date(df[[i]])
}

var_cont <- c("MONETARY_AMOUNT_DEMANDED", "AMOUNT_RECEIVED" )

for (i in var_cont){
  df[[i]] <- as.numeric(df[[i]])
}


df$OPEN_DAYS <- df$TERMINATION_DATE - df$FILING_DATE
df$OPEN_DAYS <- as.numeric(df$OPEN_DAYS)

ggplot(df, aes(x=OPEN_DAYS)) +
  geom_histogram(binwidth = 500, fill ="cyan3") +
  xlab('Open Days') + ylab('Frequency')+ ggtitle('Frequency Distribution: Open Days for All Cases')+
  theme(plot.title = element_text(hjust = 0.5))

closed_df <- subset(df, STATUS_CODE=='L')  # the rest are not terminated yet, 1900-01-01

ggplot(closed_df, aes(x=OPEN_DAYS)) +
  geom_histogram(binwidth = 500, fill ="cyan3", bins = 100) + 
  xlab('Open Days') + ylab('Frequency')+ ggtitle('Frequency Distribution: Open Days for Closed Cases')+
  theme(plot.title = element_text(hjust = 0.5))

ggplot(closed_df, aes(y=OPEN_DAYS, x = 0)) +
  geom_boxplot(fill ="cyan3", color = 'black', width=0.2) +
   ylab('Open Days')+xlim(-0.5,0.5) + ggtitle('Box Plot: Open Days for Closed Cases')+
  theme(plot.title = element_text(hjust = 0.5))

ggplot(closed_df, aes(y=OPEN_DAYS, x = 0)) +
  geom_boxplot(fill ="cyan3", color = 'black', width=0.2) +
   ylab('Open Days')+xlim(-0.5,0.5) + ggtitle('Box Plot: Open Days for Closed Cases')+
  theme(plot.title = element_text(hjust = 0.5)) + ylim(0,2500)

y_hist <- hist(closed_df$OPEN_DAYS,breaks=100)
print(y_hist)
summary(closed_df$OPEN_DAYS)
describe(closed_df$OPEN_DAYS) 
```



## 2. Question 2: 

How is “Days Open” related to a case’s Circuit (CIRCUIT) and the year the case was closed (YEAR OF TAPE)?

Open Days is a dependent variable, referred to a (Y) and the remaining variables are independent variables (Xs). CIRCUIT is a categorical variable with a total of 12 levels. YEAR OF TAPE is the year at which cases are terminated. Year of tape is listed as 2099 for the pending cases. 
It is better to consider YEAR OF TAPE as a categorical variable, since YEAR OF TAPE for pending cases is not meaningful, and its magnitude does not represent meaningful values. YEAR OF TAPE has seven levels with level 2099 associated to pending cases. 
I am using three techniques to identify how Days Open is related to Circuit and Year Of Tape: 1) Visualization, 2) correlation Analysis, and 3) hypothesis testing.

1) Visualization:

Plotting Open Days with respect to YEAR OF TAPE for all data in plot “Open Days for All Cases per Year of Tape”, clearly separates the pending cases in year label ‘2099’ and negative values. Since we do not have data on the number of open days for pending cases, I use only closed cases to find how Open Days is related to CIRCUIT and YEAR OF TAPE.
Plots “Open Days for Closed Cases per Year of Tape” and “Density Distribution of Open Days per Year of Tape”  show how number of Open Days changes per category of Year of Tape. Both plots show similar Open Days per years.
Plots “Open Days for Closed Cases per Circuit” and “Density Distribution of Open Days per Circuit” show how number of Open Days changes per category of Circuit. Circuit 4 has greater number of Open days.the distribution of open days in Circuit 4 is more disperesed than in the other circuits. Circuits with greater Open Days include cirsuit 4, followed by Circuit 1 and 7. 
Plot “Mean of Open Days for each Circuit and Year of Tape” looks at the mean number of open days where both year of tape and circuit change. For example circuit 4 and years of 2016, 2017, and 2018 in an increasing order, have higher mean.
Plots of “'Median of Open Days for each Circuit and Year of Tape'” similarly, plots the median of open days when year of tape and circuit interact. Circuit 4 in years of 2016, 2017, and 2018 have positive relationship with Open Days, in an increasing order. They Also have greater median than other levels in Year of Tape and Circuit. Same is true for circuit 8 in years 2013 and 2014, Circuit 7 in year 2015,Circuit 1 in year 2015, and Circuit 10 and year 2018.
Plots “'Box Plot: Open Days for each Circuit and Year of Tape” and “Box Plot: Open Days for each Circuit and Year of Tape” shows boxplot from the number of open days of cases in when year of tape and circuit change. Although we can see the median here to, it is a good way to captures the distribution of cases’ open days.

2) Correlation Analysis:

I look at the correlation between Open Days with Year of Tape and Year Of Tape indivitually and in combination. In order to find the correlation I use Biserial correlation as I need the correlation between a continuous and a categorical variable.

Individual: I look at the correlation between Open Days with Year of Tape. I also look at the correlation between Open Days with Circuit levels.The results are in plot “Biserial Correlation with Circuit and Year of Tape”. Circuits, 1, 4, 6, 7, 8 have positive relationship with number of open days. Therefore cases in circuit 1, 4, 6, 7, 8 have greater number of open days.
Also, years 17 and 18 have positive relationship with number of open days. Cases Closed on years 17 and 18 have greater number of open days.

Interaction: I calculate the Biserial correlation for every combination of Year of Tape and Circuit. The results confirm the boxplots from the visualization section, where cases that are in the Circuit 4 and years of 2016, 2017, and 2018 have the highest positive relationship with Open Days.
It can be seen that Circuit 9, 10, and 11 have negative relation with the number of open days in all circuit levels. Also Circuit 2 have negative relation with the number of open days in all circuit levels.
Circuit 8 in years 2013 and 2014, Circuit 6 and 7 in year 2015, Circuit 1 in year 2015. Circuit 10 in year 2018 follow with the highest positive relation.

3) Hypothesis Testing: 

Based on the histogram in question 1 and also the density plots in the visualization section, we know that the distribution of Open Days is not normal, and using standard one-way anova test is not justified.
Kruskal-Wallist test is a non-parametric test that investigates if the parameters of distribution of open days across different levels of Circuit is equal. The null hypothesis is rejected as the resulting p-value < 2.2e-16. 
Now let’s test if the parameters of the distribution of yearly open days is equal. Again, the p-value < 2.2e-16 rejects this hypothesis.
CIRCUIT_YEAR_OF_TAPE is new categorical variable that shows all combinations of Year of Tape and Circuit. It has 72 levels (12*6=72). In this section I test if the parameters of distribution of open days across all these 72 levels is equal.  
Therefore, the parameters of at least one of the distribution of open days across all these 72 levels is not equal. The p-value < 2.2e-16 rejects this hypothesis. The value of the parameter of the open days distribution differs at least one of the categories. 

```{r Q2, out.width = "50%",  fig.show='hold'} 
yot <- as.numeric(as.numeric_version(df$YEAR_OF_TAPE))
print (c("Min of Year of Tape: ", min(yot), "Max of Year of Tape: ", max(yot)))
#  all cases:
ggplot(df, aes(y=OPEN_DAYS, x = YEAR_OF_TAPE, fill = YEAR_OF_TAPE)) +  ggtitle('Open Days for All Cases per Year of Tape')+
  theme(plot.title = element_text(hjust = 0.5))+
  geom_boxplot()+xlab('Year of Tape')+ylab('Open Days')


ggplot(closed_df, aes(y=OPEN_DAYS, x = YEAR_OF_TAPE, fill = YEAR_OF_TAPE)) +  ggtitle('Open Days for Closed Cases per Year of Tape')+
  theme(plot.title = element_text(hjust = 0.5))+
  geom_boxplot()+xlab('Year of Tape')+ylab('Open Days')

ggplot(closed_df, aes(y=OPEN_DAYS, x = YEAR_OF_TAPE, fill = YEAR_OF_TAPE)) +  ggtitle('Open Days for Closed Cases per Year of Tape (Zoomed)')+
  theme(plot.title = element_text(hjust = 0.5))+
  geom_boxplot()+xlab('Year of Tape')+ylab('Open Days') + ylim(0,2500)

ggplot(df, aes(y=OPEN_DAYS, x = CIRCUIT, fill = CIRCUIT)) +  ggtitle('Open Days for All Cases per Circuit')+
  theme(plot.title = element_text(hjust = 0.5))+
  geom_boxplot()+xlab('Circuit')+ylab('Open Days')


ggplot(closed_df, aes(y=OPEN_DAYS, x = CIRCUIT, fill = CIRCUIT)) +  ggtitle('Open Days for Closed Cases per Circuit')+
  theme(plot.title = element_text(hjust = 0.5))+
  geom_boxplot()+xlab('Circuit')+ylab('Open Days')

ggplot(closed_df, aes(y=OPEN_DAYS, x = CIRCUIT, fill = CIRCUIT)) +
  geom_boxplot() + ylim(0,2500) +  ggtitle('Open Days for Closed Cases per Circuit (Zoomed)')+
  theme(plot.title = element_text(hjust = 0.5))+
  geom_boxplot()+xlab('Circuit')+ylab('Open Days')


ggplot(closed_df, aes(closed_df$OPEN_DAYS)) + 
  geom_density(aes(data = closed_df$OPEN_DAYS, fill = closed_df$CIRCUIT), position = 'identity', alpha = 0.5) +
  labs(x = 'Circuit', y = 'Density') + scale_fill_discrete(name = 'CIRCUIT')+  ggtitle('Density Distribution of Open Days per Circuit')+
  theme(plot.title = element_text(hjust = 0.5))

ggplot(closed_df, aes(closed_df$OPEN_DAYS)) + 
  geom_density(aes(data = closed_df$OPEN_DAYS, fill = closed_df$YEAR_OF_TAPE), position = 'identity', alpha = 0.5) +
  labs(x = 'Year of Tape', y = 'Density') + scale_fill_discrete(name = 'YEAR_OF_TAPE')+  ggtitle('Density Distribution of Open Days per Year of Tape')+
  theme(plot.title = element_text(hjust = 0.5))

# grouped <- closed_df %>%
#   dplyr::group_by(CIRCUIT, YEAR_OF_TAPE) %>%
#   dplyr::summarise(
#     Mean=mean(OPEN_DAYS),
#     Median = median(OPEN_DAYS), SD = sd(OPEN_DAYS))

grouped <- read.csv('./grouped.csv')


ggplot(grouped, aes(x=CIRCUIT, y=YEAR_OF_TAPE)) + geom_point(aes(size = Mean), color='cyan3')+
  xlab('Circuit') + ylab('Year of Tape') + ggtitle('Mean of Open Days for each Circuit and Year of Tape')+
  theme(plot.title = element_text(hjust = 0.5)) 


ggplot(grouped, aes(x=CIRCUIT, y=YEAR_OF_TAPE)) + geom_point(aes(size = Median), color='orange')+
  xlab('Circuit') + ylab('Year of Tape') + ggtitle('Median of Open Days for each Circuit and Year of Tape')+
  theme(plot.title = element_text(hjust = 0.5)) 


ggplot(closed_df, aes(x=CIRCUIT, y=OPEN_DAYS, color=YEAR_OF_TAPE)) +
  geom_boxplot(position=position_dodge(0.9), outlier.size = 0.5)+
  ylim(0,6000) + xlab('Circuit')+ylab('Open Days') +  ggtitle('Box Plot: Open Days for each Circuit and Year of Tape')+
  theme(plot.title = element_text(hjust = 0.5))


ggplot(closed_df, aes(x=YEAR_OF_TAPE, y=OPEN_DAYS, color=CIRCUIT)) +
  geom_boxplot(position=position_dodge(0.9), outlier.size = 0.5)+
  ylim(0,6000) + xlab('Year of Tape')+ylab('Open Days') +  ggtitle('Box Plot: Open Days for each Circuit and Year of Tape')+
  theme(plot.title = element_text(hjust = 0.5))

# 2)  Correlation Analysis

# q2_data <- closed_df[,colnames(closed_df) %in% c("CIRCUIT","YEAR_OF_TAPE", "OPEN_DAYS")]
# q2_data_bm <- createDummyFeatures(q2_data, cols = c("CIRCUIT","YEAR_OF_TAPE"))
# 
# names<-names(q2_data_bm)[2:19]
# 
# 
# cor_df <- data.frame(cor = 1:18)
# row.names(cor_df) <- names
# 
# for (i in names){
#   cor_df[i, 'cor'] <- biserial.cor(as.numeric(q2_data_bm$OPEN_DAYS), q2_data_bm[[i]])
# }

cor_df <- read.csv('./cor_df_single.csv')
cor_df$cor <- cor_df$cor*(-1)
cor_df$sign <- ifelse(cor_df$cor > 0, 'Positive', 'Negative')

ggplot(cor_df, aes(x=X, y=cor)) +
  geom_point(aes( color=sign)) + theme(axis.text.x = element_text(size=8, angle=90))+
  xlab(' ')+ ylab('Correlation')+ ggtitle('Point-Biserial Correlation with Circuit and Year of Tape')+
  theme(plot.title = element_text(hjust = 0.5))



closed_df$CIRCUIT_YEAR_OF_TAPE <- with(closed_df, interaction(CIRCUIT,  YEAR_OF_TAPE), drop = TRUE )
# q2_data <- closed_df[,colnames(closed_df) %in% c("CIRCUIT_YEAR_OF_TAPE", "OPEN_DAYS")]
# q2_data_bm <- createDummyFeatures(q2_data, cols = c("CIRCUIT_YEAR_OF_TAPE"))
# 
# names<-names(q2_data_bm)[2:73]
# 
# 
# cor_df <- data.frame(cor = 1:72)
# row.names(cor_df) <- names
# 
# for (i in names){
#   cor_df[i, 'cor'] <- biserial.cor(as.numeric(q2_data_bm$OPEN_DAYS), q2_data_bm[[i]])
# }
# 
# cor_df$CIRCUIT_YEAR_OF_TAPE <- names
# cor_df$CIRCUIT <- substr(cor_df$CIRCUIT_YEAR_OF_TAPE, nchar(cor_df$CIRCUIT_YEAR_OF_TAPE)-7+1, nchar(cor_df$CIRCUIT_YEAR_OF_TAPE)-5)
# 
# cor_df$YEAR_OF_TAPE <- substr(cor_df$CIRCUIT_YEAR_OF_TAPE, nchar(cor_df$CIRCUIT_YEAR_OF_TAPE)-4+1, nchar(cor_df$CIRCUIT_YEAR_OF_TAPE))
# 
# 
# cor_df$Correlation <- abs(cor_df$cor)
# cor_df$sign <- ifelse(cor_df$cor > 0, 'Negative','Positive')

cor_df <- read.csv('./cor_df_interaction.csv')
cor_df$CIRCUIT <- as.character(cor_df$CIRCUIT)

ggplot(cor_df, aes(x=CIRCUIT, y=YEAR_OF_TAPE)) + geom_point(aes(size = Correlation, color = sign))+
  xlab('Circuit') + ylab('Year of Tape') + ggtitle('Point-Biserial Correlation for each Circuit and Year of Tape')+
  theme(plot.title = element_text(hjust = 0.5))


# 3)  Hypothesis Testing:

kruskal.test(OPEN_DAYS ~  CIRCUIT , data = closed_df)
kruskal.test(OPEN_DAYS ~  YEAR_OF_TAPE , data = closed_df)

kruskal.test(OPEN_DAYS ~  CIRCUIT_YEAR_OF_TAPE , data = closed_df)

```


## Question 3: 

Knowing the expected Days Open for a case when it is filed could be very helpful for litigation management. Could you build a model using closed cases that predicts the Days Open for pending federal cases? How did your model perform on a holdout of closed cases? Why did you choose this model over other approaches? Tip: In the dataset, the variable STATUS_CODE indicates whether the case is closed (L) or pending (S). Think about how the status of the case might affect the choice of predictors in your model.

Data Preperation and Feature Engineering:
After Checking missing values, value ranges, and outliers, I prepare my data by keeping the useful variables in keep_var. Although, The only variable with NA values is TRANSFER_DATE, based on the codebook, “-8” stands for missing values. "DIVERSITY_RESIDENCE","PLAINTIFF", "DEFENDANT","FEE_STATUS" are columns in data that include “-8”. Since all of these four variables are factor variables, I get “-8” as another separate category. I will look into each of the four columns later in the section to reduce their category levels are elliminate them. 

Column id, includes unique values and does not provide any information for predicting Open Days. Columns STATUS_CODE, DISPOSITION, PROCEDURAL_PROGRESS, AMOUNT_RECEIVED, JUDGMENT, NATURE_OF_JUDGMENT, and TERMINATION_DATE are eliminated as they not useful in prediction model for Open Days. For example procedural process of pending cases are associated to "-8" while its levels in closed cases includes 13 non-missing valu categories. As a result,it is not useful for the prediction and is excluded from the model. Also AMOUNT_RECEIVED is information that will be known after a case is closed. other columns, either represent future information, or are not useful, or leak information. The column DIVERSITY_RESIDENCE is eliminated as it is defined for only 104601 observations.

Eliminating Dates: 
Since we are only looking into the closed cases in training models, therefore, using filing dates as a predictor biases the results. for example, considering we only use closed cases for training, the later filing dates would associate to least open days. , YEAR_OF_TAPE is also elliminated with the same logic. TRANSFER_DATE is a date that is dependent to the filing date. I generate a binary variable with value 1 if a case has been transferred, and 0 otherwise.

Reducing Categories:
COUNTY_OF_RESIDENCE and MONETARY_AMOUNT_DEMANDED are two predictors that their levels are merged based on the definitions in the codebook. For example, for COUNTY_OF_RESIDENCE, values of 88888 are renamed as category “3” and 99999 as category “4”. Those whose plaintiff is US Government are found from the plaintiff column and renamed as “1” and the rest of the counties are category “2”. Now the number of categories is reduced to 4 from 3052.
For the MONETARY_AMOUNT_DEMANDED, the codebook states that values above 10,000 are listed as 9999. Dollar figure is rounded to the nearest thousand and that data may not be accurate. Plot “Histogram: Monetary Amount Demanded” shows that most of the data is in the first bin. Plot “Open Days vs. Monetary Amount Demanded” shows that the average number of Open Days for greater Monetary Amount Demanded is slightly decreasing.  Therefore, due to non-accuracy of data and distribution of the variable, I categories this continuous variable into 3 categories as “1” for less than or equal to 2000, “2” for greater than 2000 and less than or equal to 8000, and “3” for greater than 8000.
I look at the histogram of a variable and how number of open days vary across different levels of the variable, and reduce the number categories. For example:
DISTRICT and PRO SE are two variables that their greatest 'p' categories are kept while the remaining are merged using 'mergelevels' function.
ORIGIN is merged based on the fact that '8', '9,'10','11','12' are all reopened cases in codebook description. After plotting the histogram and extracting number of cases in each category, categories "3","7",'13' have few observations and are merged.

Final Set of Predictors,  Model Development, Performance Metrics :
Vector keep_var has all the variables that are used for a prediction model. All predictors are categorical. Data frame “data” represents the total training set including closed cases and useful variables. As explained in question 1, there are few cases with number of open days greater than 6000, which are eliminated as outliers. 
Training and Testing Data:
75% of data is randomly selected for training prediction models and the rest will be unseen by models to test the performance of models.
Model Based on Response Variable and Data characters, and Model Assumptions: 
GLM:
The number of Open Days is of count type and is bounded to be equal or greater than zero. The linear model is not a good option as it treats the response variable as a continous variable, and in prediction produces negative values. Alternatively, after fitting a linear model, and plotting the error terms, it is clear that the errors do not have equal variance across fitted values. The Q-Q plot is also far from the diagnal line and therefore, the distribution of errors is not normal. The data does not have the linear regression model's assumption. The performance of the model is shown both on the training set to see the fit measure, and on the unseen test data to capture the test error. RMSE of 388.8285, shows the average of squared differences between prediction and actual open days. A R-squared of 26% shows that using this model, we have 26% improvemnet in our predictions than simply using the average of open days of trainig data. AIC also is a metric useful for comparing multiple models, particularly models with different number of variables. AIC of the linear regression is 4,145,711.
A generalized Linear Model (GLM) with Poisson link function does not have the restrictive assumptions of the linear regression model. However, after fitting the Poisson GLM model and Chi-square test shows that the Poisson model does not fit the data. In order to keep the record of model metrics, the R-squares of 24.5%, RMSE of 392.94, and AIC of 86,851,400 are improved in the Poisson model compared to the linear regression model. 
Investigating the reason for why Poisson model does not fit the data. The result of the dispersion test favors the alternative hypothesis that the true dispersion is greater than 1. Therefore, a negative binomial or zero-inflated model are more appropriate. 
Following the same logic as the Poisson GLM, a Negative Binomial Model is fitted as tested through chi-squared test. The test again suggests that the negative binomial model does not fit the data. The dispersion may not be fully captured by negative binomial model. 
ZNB:
Zero-Inflated Negative Binomial Model (ZNB) fits the data and resolves the dispersion and zero-inflation. ZNB is a two phase method where part of cases with zero Open Days are used in a binary classification model to identify the excess zeros. This part of model is named zero model. The part that trains and predicts the number of Open Days is the count model. The ZNB model performance metrics are as follows. The model AIC of 3,817,377 is improved compared to previous models, however, its prediction performance on test data is worsened, shown by RMSE of 401.98 and R-squares 21%.
The Vuong test compares the zero-inflated model with an ordinary Negative Binomial regression model. Here, the test statistic is significant, indicating that the zero-inflated model is superior to the standard Negative Binomial model.

Now moving to machine learning methods, tree-based methods are another way to develop regression models. Trees segment the predictor space into a number of regions. Each time, the tree is split into two, in a way that returns the greatest improvement is a loss function such as residual sum of squares for a regression model. Tree based models do not have any assumptions and can capture interactions between variables. They tend to perform well with categorical variables but can become computationally expensive as more categories per predictor results in more branches. Using rpart library, I grow a tree on trainig data as large as possible. Then I prune it through tuning the cp (complexity parameter) parameter. 
Tree:
The model “Tree” is a simple tree trained on the train data. The Mean Squared Error of the Tree Model is 129,890.68. This is the MSE of the fitted values versus actual values in the training data. In terms of its performance on the unseen portion of data, the Tree model reduces the test error metric of RMSE to 355.03, and improves R2-squared to %38. Now using a tree model, the predictions would be 38% better than simply using the average of the open days. 
Using a tree can overfit the data in the training phase, it is often useful to prune the tree and re-evaluate the performance of model on the unseen test data. I plot how relative errors are changing for different levels of cp values. Then I extract the value of cp at associated to the minimum error. Since error curve becomes saturated, I choose the minimum level of cp that saturates the relative error to prune the tree. As expected, the MSE of the model is increases to 130,258.06 as it is on the training portion. 
If the Tree model is overfitting, we expect that the Pruned Tree Model reduces the test error. In terms of its performance on the unseen portion of data, the test error of RMSE for the Pruned Tree Model remains approximately the same 355.51. The same is true for the R2-squared of %38. 
So pruning has reduced the complexity of the model without sacrificing from its predictive performance and test error. By comparing the actual versus predictions plot of the Pruned Tree model with Zero-Inflated Negative Binomial Model, many points at the top left corner of the ZNB plot are now vanished. The dispersion of points are more directed towards the diagonal line that represents a perfect prediction.
Following the same logic, I use Random Forest Model (RF) to strategically reduce overfitting and add a little bit of noise to the model. RF is an ensemble learning method that captures different subsets of variables and observations from the training data. It trains a tree on each subset and outputs the mean of the Open Days at each ending node (leaf) of the subset tree. Since the number of cases are 372,680 in the closed cases data, I use H2O library to train a random forest. Initially, I run a RF model with default parameters, to use as a bench mark for my best resulting model from RF. This benchmark model has a model MSE of 218725.18., and testing error RMSE of 366.31. 
RF:
RF has a number pf parameters that can be tuned. I create a grid search that captures all combinations of tuning parameters. I use ntrees, max_depth, mtries, and sample_rate as my tuning parameters. The best model has a training MSE of 126,118.6, which is improved compared to the benchmark model. In terms of the performance on the unseen portion of data, the testing error of RMSE is 361.31. The R-squared of the predictions is 36%. Both the RMSE and R2-squared are better than the benchmark RF model but not as good as the Pruned Tree Model. Comparing the Actual versus predicted plot for the Best RF and Pruned Tree, we see that points in the Pruned Tree Model are placed closer to the diagonal line compared to the best Random Forest.
GBM:
Next Model to investigate is the Gradient Boosting Machines (GBM). Unlike random forest, Gradient Boosting trains several models in a sequential way and adds them gradually. The parameters used for the grid search for my GMB model are learning rate which is the step size by which models are added. Similar to RF it also uses max_depth and sample_rate. Here,  instead of mtries in RF,we can use column_sample_size as another way to sample from variables.  
While the training MSE is less than that of RFm the testing Error RMSE and prediction R-squared are almost equal.  
XGBoost
At last, I use the eXtreme Gradient Boosting (XGBoost) model from caret library to fit on training data. caret automates and optimizes required steps for developing a model through arguments of the 'train' function and 'trainControl'  functions. XGBoost also uses the gradient boosting concept. The difference, however, is that it uses a more regularized model formalization to deal with overfitting. I use 'trainControl' function to determine the training step. I specify using repeated cross-validation with grid search for 5 times.
As you can see in the results, the testing error has remained almost equivalent to that of RF and GBM model. Comparing the actual versus predicted plots, we can find groups of points where a GBM is a better prediction. For instance when Open days is around 3000 days. Also, when a RF provides a better prediction. For instance when Open days is around 1800 days. However as the performance metrics of models indicate, the spread of points in these plot are similar for RF, GBM, and XGBoost too. 
After trying multiple models, I plot the performance measures for all models to compare their performance. We can see that Tree Model has the lowest test error with a slight difference with the Pruned Tree Model. They both take short amount of time compared to other RF, GBM, and XGboost models. The Pruned Tree Model is the least complex model that provides the best prediction performance. It also can be easily plotted to better understand the branches. 

```{r data_prepration, out.width = "50%",  fig.show='hold'} 
closed_df <- subset(df, STATUS_CODE=='L')  # the rest are not terminated yet, 1900-01-01


for (i in names(closed_df)){
  if ("-8" %in% unique(closed_df[[i]])){
    print(i)}
  
}


indx <- apply(closed_df, 2, function(x) any(is.nan(x)| is.na(x) | is.infinite(x)))
names(closed_df)[indx]   # "TRANSFER_DATE"
closed_df$TRANSFER_DATE_CAT <- ifelse(closed_df$TRANSFER_DATE == as.Date("1931-01-01") | is.na(closed_df$TRANSFER_DATE), 0, 1)
closed_df$TRANSFER_DATE_CAT <- as.factor(closed_df$TRANSFER_DATE_CAT)


ggplot(closed_df, aes(x=MONETARY_AMOUNT_DEMANDED)) +
  geom_histogram(alpha=0.5, position="identity", fill = 'cyan3') +  ggtitle('Histogram: Monetary Amount Demanded')+
  theme(plot.title = element_text(hjust = 0.5))

ggplot(closed_df, aes(x=MONETARY_AMOUNT_DEMANDED, y=OPEN_DAYS)) + geom_point(color = 'cyan3')+
  xlab('Monetary Amount Demanded')+ ylab('Open Days') + ggtitle('Open Days vs. Monetary Amount Demanded')+
  theme(plot.title = element_text(hjust = 0.5))


closed_df$MONETARY_AMOUNT_DEMANDED_CAT <- ifelse(closed_df$MONETARY_AMOUNT_DEMANDED <= 2000, 1,
                                                 ifelse(closed_df$MONETARY_AMOUNT_DEMANDED <= 8000 , 2,3))

closed_df$MONETARY_AMOUNT_DEMANDED_CAT <- as.factor(closed_df$MONETARY_AMOUNT_DEMANDED_CAT)


closed_df$DIVERSITY_RESIDENCE <- as.character(closed_df$DIVERSITY_RESIDENCE)
closed_df$DIVERSITY_RESIDENCE_P <- ifelse(closed_df$DIVERSITY_RESIDENCE != "-8", substring(closed_df$DIVERSITY_RESIDENCE, 1,1), "-8")
closed_df$DIVERSITY_RESIDENCE_D <- ifelse(closed_df$DIVERSITY_RESIDENCE != "-8", substring(closed_df$DIVERSITY_RESIDENCE, 2), "-8")
closed_df$DIVERSITY_RESIDENCE_P <- as.factor(closed_df$DIVERSITY_RESIDENCE_P)
closed_df$DIVERSITY_RESIDENCE_D <- as.factor(closed_df$DIVERSITY_RESIDENCE_D)



closed_df$ORIGIN_CAT <- ifelse(closed_df$ORIGIN %in% c("8","9",'10','11','12'), "7", 
                               ifelse(closed_df$ORIGIN%in% c("3","7",'13'), '3',closed_df$ORIGIN))   # 7: reopened/////////// 3: 3,7,13

closed_df$ORIGIN_CAT <- as.factor(closed_df$ORIGIN_CAT)

theTable <- within(closed_df, 
                   DISTRICT <- factor(DISTRICT, 
                                      levels=names(sort(table(DISTRICT), 
                                                        decreasing=TRUE))))

ggplot(theTable, aes(x=DISTRICT)) +
  geom_histogram(fill ="cyan3", stat = 'count') + theme(axis.title.x=element_blank(),
                                                     axis.text.x=element_blank(),
                                                     axis.ticks.x=element_blank()) +
  xlab('District')  + ggtitle('Histogram: Districts')+
  theme(plot.title = element_text(hjust = 0.5))

mergelevels <- function(vec, p=16, newname='other', levs){
  t <- vec
  if (!is.character(t)){
    t <- as.character(t)
  }
  t[t == "-8" | is.na(t) | t == ""] <- "missing"
  t <- gsub("[^[:alnum:] ]", "", t)
  t_levs <- names(sort(table(t), decreasing=T))
  dims <- length(t_levs)
  
  if (!missing(levs)) {
    newlevs <- setdiff(t_levs, c(levs, NA_character_))
    if (length(newlevs) > 0) 
      ord <- unique(c(levs, newname))
    else
      ord <- levs
    
  } else if (dims >= p){
    keeplevs <- t_levs[1:p]
    newlevs <- setdiff(t_levs, c(keeplevs, NA_character_))
    ord <- c(sort(keeplevs), newname)
    # make other category as the last level
  } else {
    vec <- factor(t, levels = t_levs)
    return(vec)
  }
  
  t[t %in% newlevs] <- newname
  vec <- factor(t, levels = ord)
  return(vec)
}

closed_df$DISTRICT_CAT <- mergelevels(closed_df$DISTRICT, p=20, newname='other')
closed_df$DISTRICT_CAT <- as.factor(closed_df$DISTRICT_CAT)

ggplot(theTable, aes(x=PRO_SE)) +
  geom_histogram(fill ="cyan3", stat = 'count') +  ggtitle('Histogram: Pro Se')+
  theme(plot.title = element_text(hjust = 0.5))


closed_df$PRO_SE_CAT <- mergelevels(closed_df$PRO_SE, p=1, newname='1')
closed_df$PRO_SE_CAT <- as.factor(closed_df$PRO_SE_CAT)


closed_df$COUNTY_OF_RESIDENCE_CAT <- ifelse(closed_df$COUNTY_OF_RESIDENCE == "88888", 3,
                                            ifelse(closed_df$COUNTY_OF_RESIDENCE == '99999' , 4,
                                                   ifelse(closed_df$PLAINTIFF == 'UNITED STATES OF AMERICA',1,2)))

closed_df$COUNTY_OF_RESIDENCE_CAT <- as.factor(closed_df$COUNTY_OF_RESIDENCE_CAT)
```

```{r data_prepration_2, out.width = "50%",  fig.show='hold'} 
# data prepration

keep_var <- c("OPEN_DAYS", "FEE_STATUS", "PRO_SE_CAT", "TRANSFER_DATE_CAT", "MONETARY_AMOUNT_DEMANDED_CAT", 
              "COUNTY_OF_RESIDENCE_CAT", 
              "JURY_DEMAND",  "ORIGIN_CAT", "DISTRICT_CAT", "CIRCUIT")

data <- closed_df[, names(closed_df) %in% keep_var]

y_hist <- hist(closed_df$OPEN_DAYS)
print(y_hist)

data <- subset(data, OPEN_DAYS <= 6000)


samp_size <- floor(0.75*nrow(data))
set.seed(1000)

train_ind <- sample(seq_len(nrow(data)), size = samp_size)
train <- data[train_ind,]
test <- data[-train_ind,]

```

```{r lm, out.width = "50%",  fig.show='hold'} 
# linear regression
lm.fit <- lm(OPEN_DAYS ~ ., data = train)
summary(lm.fit)
plot(lm.fit, 1)
plot(lm.fit, 2)

p <- predict(lm.fit, train[, !(colnames(train) %in% c("OPEN_DAYS"))])
print(c("MSE: ", MSE(p, train$OPEN_DAYS)))



p <- predict(lm.fit, test[, !(colnames(test) %in% c("OPEN_DAYS"))])


model_performance<-function(name, mod,  pred,actual, aic = TRUE){
  rmse <- RMSE(pred, actual)[[1]]
  r2 <- R2_Score(pred, actual)
  
  if (aic == FALSE){aic <- NA}
  else{aic <- AIC(mod)}

  out <- data.frame(name, aic, r2, rmse)
  names(out) <- c("Model", "AIC",  "R2" , "RMSE" )
  return(out)
}

lm_perf <- model_performance('lm', lm.fit, p, test$OPEN_DAYS)
print(model_performance('lm', lm.fit, p, test$OPEN_DAYS))


```

```{r glm.pois, out.width = "50%",  fig.show='hold'} 
# glm: poisson

glm.pois <- glm(OPEN_DAYS ~ ., data = train, family = 'poisson')
summary(glm.pois)
glm.pois <- readRDS('glm.pois.rds')

p_train <- predict(glm.pois, train[, !(colnames(train) %in% c("OPEN_DAYS"))], type = 'response')
R2_Score(p_train, train$OPEN_DAYS)
p <- predict(glm.pois, train[, !(colnames(train) %in% c("OPEN_DAYS"))])
print(c("MSE: ", MSE(p, train$OPEN_DAYS)))

p_test <- predict(glm.pois, test[, !(colnames(test) %in% c("OPEN_DAYS"))], type = 'response')

glm.pois_perf <- model_performance('glm.pois', glm.pois,  p_test, test$OPEN_DAYS)
glm.pois_perf

1 - pchisq(summary(glm.pois)$deviance, summary(glm.pois)$df.residual)   
dispersiontest(glm.pois)

```


```{r glm.nbinom, out.width = "50%",  fig.show='hold'} 
# glm.nbinom <- glm.nb(OPEN_DAYS ~ ., data = train)
# saveRDS(glm.nbinom, 'glm.nbinom.rds')
glm.nbinom <- readRDS('glm.nbinom.rds')
summary(glm.nbinom)


p_train <- predict(glm.nbinom, train[, !(colnames(train) %in% c("OPEN_DAYS"))], type = 'response')
R2_Score(p_train, train$OPEN_DAYS)
p <- predict(glm.nbinom, train[, !(colnames(train) %in% c("OPEN_DAYS"))])
print(c("MSE: ", MSE(p, train$OPEN_DAYS)))

p_test <- predict(glm.nbinom, test[, !(colnames(test) %in% c("OPEN_DAYS"))], type = 'response')

glm.nbinom_perf <- model_performance('glm.nbinom', glm.nbinom, p_test, test$OPEN_DAYS )

print(glm.nbinom_perf)


1 - pchisq(summary(glm.nbinom)$deviance, summary(glm.nbinom)$df.residual)   

```


```{r glm.nb.inf, out.width = "50%",  fig.show='hold'} 
# zero-inflated neg binomial

# glm.nb.inf <- zeroinfl(OPEN_DAYS ~ ., data = train, dist="negbin")
# saveRDS(glm.nb.inf, "glm.nb.inf.rds")
glm.nb.inf <- readRDS("glm.nb.inf.rds")
summary(glm.nb.inf)   # log(theta) is significant, an indication for the presence of dispersion.

p_train <- predict(glm.nb.inf, train[, !(colnames(train) %in% c("OPEN_DAYS"))], type = 'response')
R2_Score(p_train, train$OPEN_DAYS)
print(c("MSE: ", MSE(p_train, train$OPEN_DAYS)))


p_test <- predict(glm.nb.inf, test[, !(colnames(test) %in% c("OPEN_DAYS"))], type = 'response')
glm.nb.inf_perf <- model_performance('glm.nb.inf', glm.nb.inf, p_test, test$OPEN_DAYS)
glm.nb.inf_perf


glm.nb.inf0 <- update(glm.nb.inf, . ~ 1)
pchisq(2 * (logLik(glm.nb.inf) - logLik(glm.nb.inf0)), df = 73, lower.tail=FALSE)   

# comparing neg binomial with zero inflated neg binomial

# vuong(glm.nb.inf, glm.nbinom)

# Vuong Non-Nested Hypothesis Test-Statistic: 
#   (test-statistic is asymptotically distributed N(0,1) under the
#    null that the models are indistinguishible)
# -------------------------------------------------------------
#   Vuong z-statistic             H_A    p-value
# Raw                    38.51545 model1 > model2 < 2.22e-16
# AIC-corrected          38.18810 model1 > model2 < 2.22e-16
# BIC-corrected          36.51017 model1 > model2 < 2.22e-16

```

```{r glm.nb.inf_2, out.width = "50%",  fig.show='hold'} 

res_df <- as.data.frame(cbind(test$OPEN_DAYS, p_test))

names(res_df) <- c('Actual', 'Predicted')

p <- ggplot(res_df, aes(Actual, Predicted))
p <- p + geom_point(color = 'cyan3', alpha=0.3) + ggtitle('Zero-inflated Negative Binomial Model') + xlab('Actual Values')+ylab('Predicted Values') +
  theme(plot.title = element_text(hjust = 0.5))
p <- p + geom_abline(intercept = 0, slope = 1)
p


```


```{r tree, out.width = "50%",  fig.show='hold'} 
# tree <- rpart(OPEN_DAYS~.,data[train_ind,], control = rpart.control(cp = 0))
# saveRDS(tree, "tree.rds")
tree <- readRDS("tree.rds")
# summary(tree)
tree.pred <- predict(tree, test)

tree_perf <- model_performance("tree", tree, tree.pred, test$OPEN_DAYS, aic = FALSE)
tree_perf
print(c("MSE: ", MSE(predict(tree, train), train$OPEN_DAYS)))
plotcp(tree)


# pruned_tree <- prune(tree, cp = tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"])
# saveRDS(pruned_tree, "pruned_tree.rds")
pruned_tree <- readRDS("pruned_tree.rds")
rpart.plot(pruned_tree, main="Pruned Decision Tree", fallen.leaves=FALSE,  box.palette="GnBu", roundint=FALSE)
pruned_tree.pred <- predict(pruned_tree,test)#,type="class")
R2_Score(pruned_tree.pred, test$OPEN_DAYS)
pruned_tree_perf <- model_performance("pruned_tree", pruned_tree, pruned_tree.pred, test$OPEN_DAYS, aic = FALSE)
print(pruned_tree_perf)
print(c("MSE: ", MSE(predict(pruned_tree, train), train$OPEN_DAYS)))


# -----------------prediction vs Actual
# tree
predict_tree <-as.data.frame(predict(pruned_tree, test))
response <- as.data.frame(test$OPEN_DAYS)
res_df <- as.data.frame(cbind(response$`test$OPEN_DAYS`, predict_tree$`predict(pruned_tree, test)`))
colnames(res_df) <- c('Actual', 'Predicted')
#colnames(res_df)[colnames(res_df)=="as.numeric(as.vector(splits[[2]]$OPEN_DAYS))"] <- "response"
# lm.model <- lm(Actual ~ Predicted, data = res_df)
# res_df <- cbind(res_df, predict(lm.model, interval = 'confidence'))
p <- ggplot(res_df, aes(Actual, Predicted))
p <- p + geom_point(color = 'cyan3', alpha=0.3) + ggtitle('Pruned Tree') + xlab('Actual Values')+ylab('Predicted Values') +
  theme(plot.title = element_text(hjust = 0.5))
p <- p + geom_abline(intercept = 0, slope = 1)
p

```


```{r RF, out.width = "50%",  fig.show='hold'} 
# ############################################################ Ensemble Learning
# initialize h2o session
h2o.no_progress()
h2o.init()

data.h2o <- as.h2o(data)
splits <- h2o.splitFrame(data.h2o,c(0.75),seed=1000) 

htrain <- h2o.assign(splits[[1]], "train.hex")   
hvalid <- h2o.assign(splits[[2]], "valid.hex")   
# --------------------------------------
y <- "OPEN_DAYS"
x <- setdiff(names(data), y) 

rf1 <- h2o.randomForest(training_frame = htrain, validation_frame = hvalid, x=x, y=y, ntrees = 1000,
                        stopping_rounds = 10,stopping_metric = "RMSE", seed = 1000000, keep_cross_validation_models= TRUE,
                        keep_cross_validation_predictions=TRUE, score_each_iteration = TRUE) 
# summary(rf1)
perf_rf1 <- h2o.performance(rf1, hvalid)
print(perf_rf1)

# ########################################### grid search RF
# rf_params1 <- list(ntrees = c(50, 100, 300, 500, 1000), max_depth = c(10,20,30,40,50), mtries= seq(2, 10, by = 2),
#                    sample_rate = c(.55, .70, .80))
# search_criteria <- list(  strategy = "RandomDiscrete",  stopping_metric = "rmse",  stopping_tolerance = 0.005,
#   stopping_rounds = 2,  max_runtime_secs = 30*60)
# h2o.grid(algorithm ="randomForest", grid_id = "rf_grid_id", x = x, y = y, training_frame = htrain,
#          validation_frame = hvalid, seed =1,search_criteria = search_criteria, hyper_params =rf_params1)
# 
# rf_gridperf <- h2o.getGrid(grid_id = "rf_grid_id",sort_by = "rmse", decreasing = FALSE)
# 
# best_rf <- h2o.getModel(rf_gridperf@model_ids[[1]])
# 
# h2o.saveModel(best_rf, 'best_rf_final')

best_rf <- h2o.loadModel("best_rf_final/DRF_model_R_1559420409492_4")
response <- as.data.frame(as.numeric(as.vector(splits[[2]]$OPEN_DAYS)))



best_rf@parameters$mtries
print(best_rf@model[["model_summary"]])

predict_rf <-as.data.frame(h2o.predict(best_rf, htrain))

print(c("MSE: ", MSE(predict_rf$predict, as.numeric(as.vector(splits[[1]]$OPEN_DAYS)))))


predict_rf <-as.data.frame(h2o.predict(best_rf, hvalid))
best_rf_perf <- model_performance("best.rf", best_rf, predict_rf, response$`as.numeric(as.vector(splits[[2]]$OPEN_DAYS))`, aic=FALSE)
print(best_rf_perf)

# -----------------------------prediction vs. actual
res_df <- as.data.frame(cbind(response$`as.numeric(as.vector(splits[[2]]$OPEN_DAYS))`, predict_rf$predict))
colnames(res_df) <- c('Actual', 'Predicted')
#colnames(res_df)[colnames(res_df)=="as.numeric(as.vector(splits[[2]]$OPEN_DAYS))"] <- "response"
# lm.model <- lm(Actual ~ Predicted, data = res_df)
# res_df <- cbind(res_df, predict(lm.model, interval = 'confidence'))
p <- ggplot(res_df, aes(Actual, Predicted))
p <- p + geom_point(color = 'cyan3', alpha=0.3) + ggtitle('Random Forest') + xlab('Actual Values')+ylab('Predicted Values') +
  theme(plot.title = element_text(hjust = 0.5))
p <- p + geom_abline(intercept = 0, slope = 1)
p

```


```{r GBM, out.width = "50%",  fig.show='hold'} 
# ################################################################################
# ############################# GBM
# gbm <- h2o.gbm(training_frame = htrain, validation_frame = hvalid, x=x, y=y,
#                ntrees=500, learn_rate=0.05, score_each_iteration = TRUE)
# 
# h2o.saveModel(gbm, 'gbm')
gbm <- h2o.loadModel('gbm/GBM_model_R_1559512812192_2')

response <- as.data.frame(as.numeric(as.vector(splits[[2]]$OPEN_DAYS)))


plot(gbm, timestep = "number_of_trees", metric = "rmse")

gbm_pred <- h2o.predict(gbm, hvalid)

gbm_perf <- h2o.performance(model = gbm,
                            newdata = hvalid)

print(gbm_perf)

# ############################ GBM grid 

# gbm_params1 <- list(learn_rate = c(0.01, 0.05, 0.1),
#                     max_depth = c(3, 5, 9),
#                     sample_rate = c(0.8, 1.0),
#                     col_sample_rate = c(0.2, 0.5, 1.0))

# Train and validate a cartesian grid of GBMs
# gbm_grid1 <- h2o.grid("gbm", x = x, y = y,
#                       grid_id = "gbm_grid1",
#                       training_frame = htrain,
#                       validation_frame = hvalid,
#                       ntrees = 200,
#                       seed = 1,
#                       hyper_params = gbm_params1)
# 
# # Get the grid results, sorted by validation rmse
# gbm_gridperf1 <- h2o.getGrid(grid_id = "gbm_grid1",
#                              sort_by = "rmse",
#                              decreasing = FALSE)
# print(gbm_gridperf1)
# 
# best_gbm <- h2o.getModel(gbm_gridperf1@model_ids[[1]])

best_gbm <- h2o.loadModel("best_gbm_final/GBM_model_R_1559420409492_5")

gbm_pred <- as.data.frame(h2o.predict(best_gbm, htrain))

print(c("MSE: ", MSE(gbm_pred$predict, as.numeric(as.vector(splits[[1]]$OPEN_DAYS)))))

best_gbm@model[["model_summary"]]
best_gbm@parameters$mtries

best_gbm_perf <- h2o.performance(model = best_gbm,
                                 newdata = hvalid)

print(best_gbm_perf)
predict_gbm <-as.data.frame(h2o.predict(best_gbm, hvalid))
best_gbm_perf <- model_performance( "best.gbm",best_gbm, predict_gbm, response$`as.numeric(as.vector(splits[[2]]$OPEN_DAYS))`, aic = FALSE)

best_gbm_perf




# ----------------------- Prediction vs. actual
predict_gbm <-as.data.frame(h2o.predict(best_gbm, hvalid))
response <- as.data.frame(as.numeric(as.vector(splits[[2]]$OPEN_DAYS)))
res_df <- as.data.frame(cbind(response$`as.numeric(as.vector(splits[[2]]$OPEN_DAYS))`, predict_gbm$predict))
colnames(res_df) <- c('Actual', 'Predicted')
#colnames(res_df)[colnames(res_df)=="as.numeric(as.vector(splits[[2]]$OPEN_DAYS))"] <- "response"
# lm.model <- lm(Actual ~ Predicted, data = res_df)
# res_df <- cbind(res_df, predict(lm.model, interval = 'confidence'))
p <- ggplot(res_df, aes(Actual, Predicted))
p <- p + geom_point(color = 'cyan3', alpha=0.3) + ggtitle('Gradient Boosting Machine (GBM)') + xlab('Actual Values')+ylab('Predicted Values') +
  theme(plot.title = element_text(hjust = 0.5))
p <- p + geom_abline(intercept = 0, slope = 1)
p

```

```{r XGBT, out.width = "50%",  fig.show='hold'} 
# ctrl <- trainControl(method = "repeatedcv",
                     # number = 5,
                     # repeats = 3, 
                     # search = "grid", 
                     # selectionFunction = "best",
                     # allowParallel = TRUE)
set.seed(849)
# caret.xgbt.model <- caret::train(OPEN_DAYS~., data=train,method = 'xgbTree',trControl = ctrl, metric = "RMSE")

# saveRDS(caret.xgbt.model,'caret.xgbt.model.rds')
caret.xgbt.model <- readRDS('caret.xgbt.model.rds')

predict_xgbt <- as.data.frame(predict(caret.xgbt.model, newdata = train))#,type='raw'
response <- as.data.frame(train$OPEN_DAYS)
print(c("MSE: ", MSE(response$`train$OPEN_DAYS`, predict_xgbt)))

# summary(caret.xgbt.model)
#------------------------------ prediction vs. actual 
predict_xgbt <- as.data.frame(predict(caret.xgbt.model, newdata = test))#,type='raw'
response <- as.data.frame(test$OPEN_DAYS)
res_df <- as.data.frame(cbind(response$`test$OPEN_DAYS`, predict_xgbt$`predict(caret.xgbt.model, newdata = test)` ))
colnames(res_df) <- c('Actual', 'Predicted')

res_df$Predicted0 <- ifelse(res_df$Predicted < 0, 0, res_df$Predicted)
xgb_perf <- model_performance("xgboost", caret.xgbt.model, res_df$Predicted0, test$OPEN_DAYS, aic = FALSE)
print(xgb_perf)

p <- ggplot(res_df, aes(Actual, Predicted0))
p <- p + geom_point(color = 'cyan3', alpha=0.3) + ggtitle('XGBoost') + xlab('Actual Values')+ylab('Predicted Values') +
  theme(plot.title = element_text(hjust = 0.5))
p <- p + geom_abline(intercept = 0, slope = 1)
p 

```


```{r model_performance, out.width = "50%",  fig.show='hold'} 
models_performance <- data.frame()
models_performance <- rbind(models_performance, as.data.frame(lm_perf))
models_performance <- rbind(models_performance, as.data.frame(glm.pois_perf) )
models_performance <- rbind(models_performance, as.data.frame(glm.nbinom_perf) )
models_performance <- rbind(models_performance, as.data.frame(glm.nb.inf_perf) )
models_performance <- rbind(models_performance, as.data.frame(tree_perf) )
models_performance <- rbind(models_performance, as.data.frame(pruned_tree_perf) )
models_performance <- rbind(models_performance, as.data.frame(best_rf_perf) )
models_performance <- rbind(models_performance, as.data.frame(best_gbm_perf) )
models_performance <- rbind(models_performance, as.data.frame(xgb_perf) )


set.seed(5)
ggplot(models_performance, aes('RMSE', RMSE)) + geom_jitter(width = 0.35, aes(colour = Model), size=3) + 
  xlab(' ') + ylab(' ')  + ggtitle('Model Performance: RMSE') +
  theme(plot.title = element_text(hjust = 0.5)) + ylab(' ')

set.seed(5)
ggplot(models_performance, aes('R2', R2)) + geom_jitter(width = 0.35, aes(colour = Model), size=3) + 
  xlab(' ') + ylab(' ')  + ggtitle('Model Performance: R2') +
  theme(plot.title = element_text(hjust = 0.5)) + ylab(' ')

```

## Question 4: 

In your model, what are the top 3 key predictors? Could you show in visualization how these variables are related to Days Open?

Generalized Linear Models provide interpretable estimated effect sizes (coefficients) that tell us how a change in a variable can change the link of the Open Days. It is very straightforward, as it provides a direction of relationship and a magnitude. For example, if coefficient of JURY_DEMANDN is  -0.56, it  would simply mean that, if the case is a JURY_DEMAND of type N, results in the decrease in the link value of Open Days by the amount of -0.56. These measures can help us in identifying the main drivers or factors that mostly impact number of Open Days. They can be used to find recommendations for improving the process. we can find on which conditions the Open days are high to find ways to resolve that problem. Or we can find the characteristics of factors, or conditions that have low Open cases. in this way we may be able to use those information on other categories. 

Comparing Feature Importance:
In GLM Zero-Inflated Negative Binomial Models, zero models predict non-occurrence of the outcome. Therefore, in the right plot shows that ORIGIN_CAT6 and ORIGIN_CAT25 are the most important predictors in getting the excess zeros. In contrast, JURY_DEMANDN, and ORIGIN_CAT3 are the most important predictors of the ZNB zero model that result in having positive number of Open Days.
The plot on the left tells us that DISTRICT_CAT47, ORIGIN_CAT6, ORIGIN_CAT2 and CIRCUIT1 have the most effect on the number of Open days. They also all positively impact the expected number of Open Days.
I am interpreting model results through Variable Importance (VI) plots, Partial Dependence Plot (PDP) and Individual Conditional Expectation (ICE) boxplot. 

VI:
I am using model libraries and objects to extract VI, I use library iml (Interpretable Machine Learning), and pdp. 
Variable importance plots tells us that what is the overall effect of a variable in predicting the dependent variable (Open days) and is captured through adding the total amount of loss that is decreased due to splits over a given predictor. But it does not tell us the direction of impact when using most machine learning methods such as Trees, RF, GBM, and XGBoost. Plots showing 'Top important variables' on the y axis for Tree Model and Pruned Tree, also plots titled "Variable Importance :DRF" and "Variable Importance :GBM" for the RF and GBM models. The plot 'Importance' at the end is for the XGBoost. These plots provide an importance measure for the averall impact of a veriable with its all levels. Tree, Pruned Tree, RF and GBM tell us that ORIGIN_CAT, DISTRICT_CAT, JURY_DEMAND, followed by CIRCUIT are the most important predictors. 

PDP:
The Partial Dependence Plot shows how mean Open Days changes with changes in a given predictor. It shows how each predictor is related to Days Open. The predictor that has the most amount in change in the mean of Open Days is more important.
The mean of Open days at ORIGIN_CAT6 is significantly larger that its other levels, indicating high relationship. 
DISTRICT_CAT has a lot of variation in the mean response across different levels. The amount of change in each category is not as large as changes in ORIGIN_CAT6,however, DISTRICT_CAT 25 is the variable factor with the largest impact in mean Open Days.  
JURY_DEMAND B and N seems to be the next variable factor with largest change in Open Days. For COUNTY_OF_RESIDENCE_CAT the category 3 have greater median.We can see that although ORIGIN_CAT is the feature with the highest importance, only category 25 among all other categories of ORIGIN_CAT have high effect of the mean response. 

ICE:
The ICE box plots tell us how each observation or case’s Open Days has changed, when a factor is changing. So the same logic holds for identifing variables with larger effect. 
For JURY_DEMAND the category N has the narrowest spread and the median is significantly smaller from other JURY_DEMAND categories. this indicates significant difference in the number of open days per individual compared to other levels in JURY_DEMAND. Therefore in JURY_DEMAND category N has high effect on open days.
 



The Top three predictors: ORIGIN_ CAT6, DISTRICT_CAT25, JURY_DEMAND B. 
The Top three variables: ORIGIN_CAT, DISTRICT_CAT, JURY_DEMAND 
Among ORIGIN_CAT, ORIGIN_CAT3 is the most important factor. The number of open days in each observation and also the mean number of Open days are highly affected by ORIGIN_CAT3. 
the same logic is true for aforementioned variables and their categories. 


```{r Q4_glm.nb.inf, out.width = "50%",  fig.show='hold', fig.width=4, fig.height=8} 
# glm zero inflated neg binomial GLM
glm.nb.inf <- readRDS("glm.nb.inf.rds")

temp <- data.frame(glm.nb.inf$coefficients)
temp <- temp[order(temp$count, decreasing = TRUE), , drop = FALSE]
temp[['X']] <- rownames(temp)
rownames(temp) <- NULL
temp$X <- factor(temp$X, levels = temp$X)
p<-ggplot(data=temp, aes(x=X, y=count)) +
  geom_bar(stat="identity",  width = 0.2 , fill = "turquoise") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = 'none')+
  labs(x = "Top important variables", y = 'Importance', 
       title="GLM Zero-Inflated Negative Binomial: Count Part")  + coord_flip() + 
  theme(plot.title = element_text(hjust = 0.5))
p

# zero model
temp <- data.frame(glm.nb.inf$coefficients)
temp <- temp[order(temp$zero, decreasing = TRUE), , drop = FALSE]
temp[['X']] <- rownames(temp)
rownames(temp) <- NULL
temp$X <- factor(temp$X, levels = temp$X)
p<-ggplot(data=temp, aes(x=X, y=zero)) +
  geom_bar(stat="identity",  width = 0.2 , fill = "turquoise") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = 'none')+
  labs(x = "Top important variables", y = 'Importance', 
       title="GLM Zero-Inflated Negative Binomial: Logit Part")  + coord_flip() + 
  theme(plot.title = element_text(hjust = 0.5))
p

```


```{r Q4_tree, out.width = "50%",  fig.show='hold'} 
# Tree
tree <- readRDS("tree.rds")
temp <- data.frame(tree$variable.importance)
temp <- temp[order(temp$tree.variable.importance, decreasing = FALSE), , drop = FALSE]
temp[['X']] <- rownames(temp)
rownames(temp) <- NULL
temp$X <- factor(temp$X, levels = temp$X)
p<-ggplot(data=temp, aes(x=X, y=tree.variable.importance)) +
  geom_bar(stat="identity",  width = 0.2 , fill = "turquoise") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = 'none')+
  labs(x = "Top important variables", y = 'Importance', title="Tree Model")  +
  coord_flip() + theme(plot.title = element_text(hjust = 0.5))
p

# Pruned Tree
pruned_tree <- readRDS("pruned_tree.rds")
temp <- data.frame(pruned_tree$variable.importance)
temp <- temp[order(temp$pruned_tree.variable.importance, decreasing = FALSE), , drop = FALSE]
temp[['X']] <- rownames(temp)
rownames(temp) <- NULL
temp$X <- factor(temp$X, levels = temp$X)
p<-ggplot(data=temp, aes(x=X, y=pruned_tree.variable.importance)) +
  geom_bar(stat="identity",  width = 0.2 , fill = "turquoise") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = 'none')+
  labs(x = "Top important variables", y = 'Importance', title="Pruned Tree Model")  +
  coord_flip() + theme(plot.title = element_text(hjust = 0.5))
p

```



```{r Q4_RF, out.width = "50%",  fig.show='hold'} 
# rf
h2o.varimp_plot(best_rf)
# gbm
h2o.varimp_plot(best_gbm)

#xgboost
xgbt_varImp <- varImp(caret.xgbt.model)
plot(xgbt_varImp, top = 20)


```

Partial Dependence Plots: Random Forest

```{r PDP_RF, out.width = "50%",  fig.show='hold'} 
#---------------------------- Partioal Dependency Plot and Ice plot
h2o.partialPlot(best_rf, data = htrain, nbins = 30)
#-----------
```

Partial Dependence Plots: GBM


```{r PDP_GBM, out.width = "50%",  fig.show='hold'} 
h2o.partialPlot(best_gbm, data = htrain, nbins = 30)

```


```{r ICE, out.width = "50%",  fig.show='hold'} 
# ----------------- ICE boxplot for the most important features
# features <- as.data.frame(splits[[2]][, !(colnames(hvalid) %in% c("OPEN_DAYS"))])
# response <- as.data.frame(as.numeric(as.vector(splits[[2]]$OPEN_DAYS)))
# 
# predfunc <- function(model, newdata)  {
#   results <- as.data.frame(h2o.predict(model, as.h2o(newdata)))
#   return(results)
# }
# 
# predictor.rf <- Predictor$new(model = best_rf,   data = features,   y = response, predict.fun = predfunc)
# predictor.gbm<- Predictor$new(model = best_gbm,   data = features,   y = response, predict.fun = predfunc)
# 
# 
# rf.org <- Partial$new(predictor.rf, "ORIGIN_CAT") %>% plot() + ggtitle("RF")
# gbm.org <- Partial$new(predictor.gbm, "ORIGIN_CAT") %>% plot() + ggtitle("GBM")
# 
# temp <- data.frame(rf.org$data$ORIGIN_CAT, rf.org$data$.y.hat)
# g <- ggplot(temp, aes(y=rf.org.data..y.hat, x = rf.org.data.ORIGIN_CAT)) +
#   geom_boxplot()  + labs(x = "ORIGIN_CAT", y = 'Open_Days', title="Random Forest")  + 
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave(file="ice_rf_boxplot_ORIGIN_CAT.png", g) 
# 
# temp <- data.frame( gbm.org$data$ORIGIN_CAT, gbm.org$data$.y.hat)
# g <- ggplot(temp, aes(y=gbm.org.data..y.hat, x = gbm.org.data.ORIGIN_CAT)) +
#   geom_boxplot() +labs(x = "ORIGIN_CAT", y = 'Open_Days', title="GBM")  + 
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave(file="ice_gbm_boxplot_ORIGIN_CAT.png", g) 
# 
# rf.cir <- Partial$new(predictor.rf, "CIRCUIT") %>% plot() + ggtitle("RF")
# gbm.cir <- Partial$new(predictor.gbm, "CIRCUIT") %>% plot() + ggtitle("GBM")
# 
# temp <- data.frame( rf.cir$data$CIRCUIT, rf.cir$data$.y.hat)
# g <- ggplot(temp, aes(y=rf.cir.data..y.hat, x = rf.cir.data.CIRCUIT)) +
#   geom_boxplot()  + labs(x = "CIRCUIT", y = 'Open_Days', title="Random Forest")  + 
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave(file="ice_rf_boxplot_CIRCUIT.png", g) 
# 
# temp <- data.frame(gbm.cir$data$CIRCUIT, gbm.cir$data$.y.hat)
# g <- ggplot(temp, aes(y=gbm.cir.data..y.hat, x = gbm.cir.data.CIRCUIT)) +
#   geom_boxplot() +labs(x = "CIRCUIT", y = 'Open_Days', title="GBM")  + 
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave(file="ice_gbm_boxplot_CIRCUIT.png", g)
# 
# 
# rf.dist <- Partial$new(predictor.rf, "DISTRICT_CAT") %>% plot() + ggtitle("RF")
# gbm.dist <- Partial$new(predictor.gbm, "DISTRICT_CAT") %>% plot() + ggtitle("GBM")
# 
# temp <- data.frame( rf.dist$data$DISTRICT_CAT, rf.dist$data$.y.hat)
# g <- ggplot(temp, aes(y=rf.dist.data..y.hat, x = rf.dist.data.DISTRICT_CAT)) +
#   geom_boxplot()  + labs(x = "DISTRICT_CAT", y = 'Open_Days', title="Random Forest")  + 
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave(file="ice_rf_boxplot_DISTRICT_CAT.png", g) 
# 
# temp <- data.frame(gbm.dist$data$DISTRICT_CAT, gbm.dist$data$.y.hat)
# g <- ggplot(temp, aes(y=gbm.dist.data..y.hat, x = gbm.dist.data.DISTRICT_CAT)) +
#   geom_boxplot() +labs(x = "DISTRICT_CAT", y = 'Open_Days', title="GBM")  + 
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave(file="ice_gbm_boxplot_DISTRICT_CAT.png", g)
# 
# 
# rf.jur <- Partial$new(predictor.rf, "JURY_DEMAND") %>% plot() + ggtitle("RF")
# gbm.jur <- Partial$new(predictor.gbm, "JURY_DEMAND") %>% plot() + ggtitle("GBM")
# 
# 
# temp <- data.frame( rf.jur$data$JURY_DEMAND, rf.jur$data$.y.hat)
# g <- ggplot(temp, aes(y=rf.jur.data..y.hat, x = rf.jur.data.JURY_DEMAND)) +
#   geom_boxplot()  + labs(x = "JURY_DEMAND", y = 'Open_Days', title="Random Forest")  + 
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave(file="ice_rf_boxplot_JURY_DEMAND.png", g) 
# 
# temp <- data.frame(gbm.jur$data$JURY_DEMAND, gbm.jur$data$.y.hat)
# g <- ggplot(temp, aes(y=gbm.jur.data..y.hat, x = gbm.jur.data.JURY_DEMAND)) +
#   geom_boxplot() +labs(x = "JURY_DEMAND", y = 'Open_Days', title="GBM")  + 
#   theme(plot.title = element_text(hjust = 0.5))
# ggsave(file="ice_gbm_boxplot_JURY_DEMAND.png", g)

knitr::include_graphics("ice_rf_boxplot_ORIGIN_CAT.png")
knitr::include_graphics("ice_gbm_boxplot_ORIGIN_CAT.png")
knitr::include_graphics("ice_rf_boxplot_CIRCUIT.png")
knitr::include_graphics("ice_gbm_boxplot_CIRCUIT.png")
knitr::include_graphics("ice_rf_boxplot_DISTRICT_CAT.png")
knitr::include_graphics("ice_gbm_boxplot_DISTRICT_CAT.png")
knitr::include_graphics("ice_rf_boxplot_JURY_DEMAND.png")
knitr::include_graphics("ice_gbm_boxplot_JURY_DEMAND.png")

```

```{r out.width = "50%",  fig.show='hold'} 


```



